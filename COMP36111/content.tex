% Set the author and title of the compiled pdf
\hypersetup{
  pdftitle = {\Title},
  pdfauthor = {\Author}
}

\section{Algorithmic Wisdom}

{\hspace{2.2em} \small Lectures 2 \& 3}

The first two lectures of the course describe different types of algorithms,
what is computable, where to optimise algorithms, asymptotics and heuristics.

\subsection{Different types of algorithms}

There are three types of algorithms that are mentioned:

\marginpar{You should know all of this from \texttt{COMP26120}.}

\begin{description}
  \item \textbf{Divide and conquer}\\
  These algorithms continually break a problem down into smaller parts, which
  are easier to solve, until eventually, the problems are trivial and easily
  solved. This is often used when the data you're operating on is in a
  recursive datastructure such as a tree. If you're writing an algorithm to
  find how many nodes there are in a tree, then you could use divide and
  conquer:

  \begin{lstlisting}[language=java]
    int countTreeSize(tree) {
    int size = 1;
    if (tree.left) size += countTreeSize(tree.left);
    if (tree.right) size += countTreeSize(tree.right);
    return size;
    }
  \end{lstlisting}

  As you can see from the example, divide and conquer algorithms are usually
  recursive.

  The divide and conquer technique can be applied to graphs, but in order to
  do this, you must keep track of which nodes you've visited with a flag on
  each node. If we wanted to count the nodes in a graph, we could do:

  \begin{lstlisting}[language=java]
    int countGraphSize(graph) {
    if (graph.visited) return 0;
    graph.visited = true;
    int size = 1;
    for (child in graph) {
      size += countGraphSize(child);
    }
    return size;
    }
  \end{lstlisting}

  \item \textbf{Mutual Recursion}\\
  Mutual recursion describes an algorithm that operates on data where one type
  of data can reference another, and the other can reference it. The example
  given is that of statements and expressions in programming languages;
  statements contain expressions, and expressions can also contain statements.
  Parsing such a structure might involve two algorithms that recurse on each
  other!

  \item \textbf{Dynamic Programming}\\
  Dynamic programming exploits the fact that when some problems are broken
  down into smaller sub-problems, some of the sub-problems are identical.
  Dynamic programming algorithms start from the very smallest sub-problems and
  build up to the final solution, and usually cache results to sub-problems in
  a table so that work is not done twice.

\end{description}

\subsection{Computability}
% TODO: Combine this with a later topic?

There are many different definitions of computability, including
\href{https://en.wikipedia.org/wiki/Lambda_calculus}{lambda calculus},
\href{https://en.wikipedia.org/wiki/Turing_machine}{Turing machines},
\href{https://en.wikipedia.org/wiki/Rewriting}{rewriting rules},
\href{https://en.wikipedia.org/wiki/Random-access_machine}{random access
machines} and (many) more. The idea that relates all of these things, is that
they all have the same capabilities. That is to say that if you can compute
something using one of these ideas, then you can also compute it on all the
others too.

There are also a class of `alternate' computing mechanisms, such as quantum
computers and neural computers. These ideas have the potential to compute things
that a Turing machine (or its equivalents cannot), but they are significantly
harder to build, and functional implementations do not exist yet.

\subsection{Asymptotics and optimisation}

When you have to get a computer to perform a task, implementing a simple
algorithm first is a good idea, since you will at least have something to
demonstrate to people, and you will gain a good understanding of the problem at
hand. However, simple algorithms are often slow; how should we evolve our
implementation to be as fast as we need it to be?

Profiling can tell you where your code is spending most of its time. Sometimes
your algorithm will be really fast, and the processor will spend most of its
time waiting for IO to give it more data; this is often the case with GPU
computation.

Assuming you find some CPU bottleneck in your code, before you spend hours
making it faster, consider whether it is worth the effort. If this part takes up
10\% of your runtime, and you make it twice as fast, your program will only run
5\% faster. This is an example of the
\href{https://en.wikipedia.org/wiki/Diminishing_returns}{Law of Diminishing
Returns}.

As well as optimising specific parts of an algorithm, you also should consider
its asymptotic run time. An algorithm that runs in $O(n^2)$ time is probably
going to be better than one that runs in $O(n log(n))$ time. However, this isn't
always the case; some algorithms (often ones with good asymptotic run times)
take a long time to set up, usually when you have to transform the data into
some different datastructure. If you are running your algorithm on a small
amount of data, then an algorithm that you can run on your data \textit{as is}
might outperform a fancier algorithm that you have to invest more overhead in.

\marginpar{Sometimes a good solution is to use different algorithms depending
on the input. If there are only a few cases that produce worst-case performance,
you could even hard-code solutions to those!}

The average case runtime of a algorithm is also important. Haskell uses a type
checker that runs in $O(2^{n^n})$ time in the worst case, but for every program
that isn't made specifically to mess with the compiler, runs in linear time.

\section{Graphs}

A graph is a pair $G = (V,E)$ where $V$ is a finite set, and $E$ is a set of
pairs between items in $V$. Elements in $V$ are \textit{vertices} or
\textit{nodes}, and elements in $E$ are \textit{edges}.

Different mathematicians have different rules about what exactly can go in a
graph. For the purposes of this course, the graphs shown in
Figure~\ref{fig:bad-graphs} aren't allowed.

\begin{figure}[h]
  \centering
  \includegraphics{diagrams/self-loops}\\
  \includegraphics{diagrams/multiple-edges}\\
  \includegraphics{diagrams/directed-graph}
  \caption{Self loops, duplicate edges and arrows are not allowed.}
  \label{fig:bad-graphs}
\end{figure}

A \textbf{directed graph} is just like a normal graph, except the edges do have
arrows. The only mathematical difference is that the set $E$ is a set of ordered
pairs.

The \textit{degree} of a node in a graph, is the number of edges that are
adjacent to (touching) it. If the graph is directed, then it is the number of
edges originating from the node.

A weighted graph is one where each edge is associated with a value representing
its weight. The length of a path between nodes is simply the sum of the edge
weights connecting the nodes.

\subsection{Connectivity}
\label{connectivity}

A node $a$ is \textbf{reachable} from another node $b$ if there is some sequence
of nodes connected by edges that go from the $a$ to $b$.

A graph where every node is reachable from every other node is
\textbf{connected}. A strongly connected graph is a \textbf{directed graph}
where each node is reachable from each other node.

\begin{figure}[h]
  \centering
  \includegraphics{diagrams/connected-graph}
  \caption{This graph is not strongly connected, but the subgraph with only
  nodes $A$ and $B$ is. If we were to remove the arrows, then the graph would
  be connected.}
  \label{fig:connected-graph}
\end{figure}

\subsection{Representing graphs}

We can store graphs in computer memory in two ways:

\begin{description}
  \item \textbf{Adjacency List}\\
    There are two different kinds of adjacency list, the first is like so:

    \begin{lstlisting}[language=java]
      public class Graph {
        List<Vertex> nodes;
        List<Edge> edges;
      }
      public class Edge { Vertex a, b; }
      public class Vertex { List<Edge> outlist; }
    \end{lstlisting}

    Here, we keep a list of all the nodes, and a list of all the edges. From
    any edge, we can see what nodes it connects, and from any node, we can
    see what edges it connects.

    The other type of adjacency list is a bit simpler, but less efficient in
    some cases:

    \begin{lstlisting}[language=java]
      public class Graph<T> {
        Set<T, List<T>> adjList;
      }
    \end{lstlisting}
  \item \textbf{Adjacency Matrix}\\
    Here, a matrix indicates whether there is an edge between two nodes:

    \begin{center}
      \begin{tabular}{c|c c c c}
          & A & B & C & D\\ \hline
        A & 0 & 1 & 0 & 0\\
        B & 1 & 0 & 0 & 1\\
        C & 0 & 1 & 0 & 0\\
        D & 0 & 0 & 0 & 0\\
      \end{tabular}
    \end{center}

    This adjacency matrix represents the graph in
    Figure~\ref{fig:connected-graph}. You can see that it is fairly wasteful
    in terms of memory $O(|E|* |V|)$, though with bit arrays, it has a very 
    low constant overhead.
\end{description}

\subsection{Classifying Edges}

During a \textit{depth first traversal} (Section~\ref{depth-first-search}) of a
graph, we can classify each edge into one of four types. When doing the
traversal, we process each edge from left to right (from the viewer's
perspective), and we define an edge to be an \textit{ancestor} of another edge
if there is a path from the ancestor edge to the descendent edge and the ancestor
is traversed first. The four types of edges are:

\begin{description}
  \item \textbf{Tree edges}\\
    These edges lead to a new node during a search. If you remove all the edges 
    from the graph except tree edges, then you get a tree!
  \item \textbf{Forward edges}\\
    These go from ancestor edges to descendent edges, but are not tree edges
    (i.e. the descendent node has been visited already).
  \item \textbf{Cross edges}\\
    These go between edges where no node is the ancestor of the other.
  \item \textbf{Back edges}\\
    An edge that goes from a descendant to an ancestor.
\end{description}

\begin{figure}[h]
  \centering
  \includegraphics[width=0.6\textwidth]{images/edge-types}
  \caption{A
  \href{http://courses.csail.mit.edu/6.006/spring11/rec/rec13.pdf}{pictorial
  illustration} of the different edge types.}
  \label{fig:connected-graph}
\end{figure}

\subsection{Graph Algorithms}

You will have covered some of the algorithms featuring here in previous courses,
or perhaps seen them in the wild. They are however interesting and useful, so
it's worth a recap even if they're not new! I've ordered these in roughly
increasing order of mental strain.

\subsubsection{Depth first search}
\label{depth-first-search}

\href{https://en.wikipedia.org/wiki/Depth-first_search}{Depth first search}
(DFS) is an algorithm to find a node in a graph starting from another node. It
works on both directed and undirected graphs, and runs in $O(|V| + |E|)$
(linear) time. The psudo code looks like this:

\marginpar{Note how we mark nodes as having been visited (by adding them to
\texttt{visitedNodes}). This is so if there is a loop in the graph, the 
algorithm doesn't run indefinitely.}

\begin{lstlisting}[language=java]
  Node dfs(Node haystack, Node needle) {
    Stack<Node> toVisit = new Stack<>();
    Set<Node> visitedNodes = new Set<>();
    toVisit.push(haystack);
    while(!toVisit.isEmpty()) {
      Node node = toVisit.pop();
      if (visitedNodes.contains(node)) continue;
      visitedNodes.add(node);
      if(node.equals(needle)) {
        return needle;
      } else {
        for(Node child : node.children) {
          toVisit.push(child);
        }
      }
    }
    return null;
  }
\end{lstlisting}

A Breadth First Search is the same, except you use a \texttt{Queue} instead of a
\texttt{Stack}.

\marginpar{To see if a graph is connected, do a depth first search as in the
example code, but don't stop when you find a needle, only stop when the
\texttt{toVisit} stack is empty. If \texttt{visitedNodes} contains all of the
nodes in the graph, then the graph is connected.}

Depth first search also lets you find if one node is reachable from another in
linear time, and also if a graph is connected in linear time too, with a few
modifications.

We can also find out if a directed graph is strongly connected in $O(|V| + |E|)$
time using Tarjan's algorithm, which we'll see later.

\subsubsection{Dijkstra's algorithm}

Dijkstra's algorithm finds the undirected shortest path between two nodes in a
graph. Here is the psudo-code:

\marginpar{My best advice for learning an algorithm like this, is to get a 
whiteboard, draw out the problem, and then run the solution manually on the 
whiteboard. Then you have a pictorial and visual explanation of how the 
algorithm \textit{really} works.}

\begin{lstlisting}[language=java,
                  caption=Dijkstra's algorithm (from Wikipedia),
                  label=lst:dijkstra,
                  captionpos=b]
function Dijkstra(Graph, source):
  create vertex set Q

  // Initialization
  for each vertex v in Graph:
    dist[v] = INFINITY
    prev[v] = UNDEFINED
    add v to Q

  // Distance from source to source
  dist[source] = 0

  while Q is not empty:
    u = vertex in Q with min dist[u]
    remove u from Q 

  for each neighbour v of u:
    alt = dist[u] + length(u, v)
    // A shorter path to v has been found
    if alt < dist[v]:
      dist[v] = alt 
      prev[v] = u 

  return dist[], prev[]
\end{lstlisting}

If we use a Fibonacci heap, for the priority queue, then the runtime of
Dijkstra's algorithm is $O(|E| + |V|log(|V|)$. If we use a normal heap, then the
runtime is $O(|E|log(|V|))$.

\subsubsection{Tarjan's Algorithm}

Before we delve into Tarjan's Algorithm, we need to discuss strongly connected
components. You will remember from Section~\ref{connectivity}, that a graph is
strongly connected if all nodes are reachable from any other node.

A \textit{strongly connected component} is a subset of edges within a graph,
where the subset is strongly connected. If a graph has only one strongly
connected component, then it is strongly connected.

The psudo code here is from
\href{https://en.wikipedia.org/wiki/Tarjan%27s_strongly_connected_components_algorithm}{Wikipedia}.
If you're reading this after week 8 of the \texttt{COMP36111} course, then you
probably have your own implementation in C.

\begin{lstlisting}
  algorithm tarjan is
  input: graph G = (V, E)
  output: set of strongly connected components (sets of vertices)

  index := 0
  S := empty
  for each v in V do
    if (v.index is undefined) then
      strongconnect(v)
    end if
  end for

  function strongconnect(v)
    // Set the depth index for v to the smallest unused index
    v.index := index
    v.lowlink := index
    index := index + 1
    S.push(v)
    v.onStack := true

    // Consider successors of v
    for each (v, w) in E do
      if (w.index is undefined) then
        // Successor w has not yet been visited; recurse on it
        strongconnect(w)
        v.lowlink  := min(v.lowlink, w.lowlink)
      else if (w.onStack) then
        // Successor w is in stack S and hence in the current SCC
        v.lowlink  := min(v.lowlink, w.index)
      end if
    end for

    // If v is a root node, pop the stack and generate an SCC
    if (v.lowlink = v.index) then
      start a new strongly connected component
      repeat
        w := S.pop()
        w.onStack := false
        add w to current strongly connected component
      while (w != v)
      output the current strongly connected component
    end if
  end function
\end{lstlisting}

%TODO: Expand on Tarjan's algorithm when a whiteboard is present

\section{Linear Programming}

Linear programming is an optimisation problem, where we want to find the `best'
solution to a set of equations. We're going to solve it using the
\textit{simplex} method, but before we do, I think it's a good idea to recap
some high-school mathematics first. Feel free to skip the next subsection if
you're feeling confident with it.

\subsection{Background mathematics}

An inequality relation is just like a normal equation, except the equals sign is
replaced by either $<$, $>$, $\leq$ or $\geq$. When you solve an inequality, you
generally want to get all of the similar terms on one side (e.g. move all the
variable terms over to one side, and all of the constants to another side). This
\textit{mostly} works like a normal equation; you can add and subtract from both
sides just like normal, however, if you want to divide or multiply \textbf{by a
negative quantity}, then you need to \textbf{reverse the equality}.

\[
  \begin{split}
    -2x &> -2\\
    2x &< 2\\
    x &< 1
  \end{split}
\]

This equality is satisfied whenever $x$ is less than $1$. If we have two terms
in the equality (something similar to $ax + by \geq c$), it becomes slightly
harder to solve. To solve this we can:

\begin{itemize}
  \item Plot a graph of the line $ax + by = c$.
  \item Pick a test point $(x, y)$ not on the line, and plot it on the graph.
  \item If the point $(x, y)$ satisfies the inequality, then shade the opposite 
    side of the line to which the point is on, otherwise, shade the same side.
\end{itemize}

For example, given $3x + 4y \leq 6$, and choosing the point $(-2,1)$ we end up
with what's in Figure~\ref{fig:graph-1}.

\begin{figure}[h]
  \centering
  \includegraphics{diagrams/graph1}
  \caption{A graph of $3x + 4y \leq 6$, with all the valid values shaded blue.}
  \label{fig:graph-1}
\end{figure}

If we have multiple inequalities, we want to \textit{find the region where all
of them are satisfied}. This involves plotting each line, and shading the
regions where they're not satisfied, which means the blank bit is the bit we
want.

If we have the equations $3x + 4y \leq 6$, $2y - x \leq 2$ and $x \geq 0$, we
will get something like in Figure~\ref{fig:graph-2}.

\begin{figure}[h]
\captionsetup{width=.4\linewidth}
\centering
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=\textwidth]{diagrams/graph2}
  \captionof{figure}{A graph plotting $3x + 4y \leq 6$, $2y - x \leq 2$ and
  $x \geq 0$, where the points not satisfying the inequalities are shaded in 
  blue, green and red respectively. The clear region satisfies all points.}
  \label{fig:graph-2}
\end{minipage}%
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=\textwidth]{diagrams/graph3}
  \captionof{figure}{The valid region in the three inequalities from
  Figure~\ref{fig:graph-2}. The corner points are $(2,0),(-2,0)$ and
  $(\frac{2}{5},\frac{6}{5})$.}
  \label{fig:graph-3}
\end{minipage}
\end{figure}


The corner points are especially important to us, since that's often where the
useful numbers are (we'll see more of this later). In order to find the corner
points, we solve the two lines that form them. Solving the following equations
gives the points in Figure~\ref{fig:graph-3}.

\begin{mymulticols}
  \begin{itemize}
    \item $x = 0, 3x + 4y = 6$
    \item $x = 0, 2y - x = 2$
    \item $2y - x = 2, 3x + 4y = 6$
  \end{itemize}
\end{mymulticols}

So, now we know how to solve these types of problems, lets look at how to
decompose a problem statement into a set of equations.

\begin{quote}

  You can buy wood in 11 meter lengths, or 6 meter lengths. A 11 meter piece of
  wood can be cut into two lengths of 5 meters, and one length of 1 meter. A 6
  meter piece of wood is cut into one length of 5 meter, and one length of 1
  meter.

  If we have room for twenty 1 meter pieces and thirty 5 meter pieces, how many
  11, and how many 6 meter lengths should we buy? We cannot go over that amount
  of pieces, but we can settle for less than that amount.

\end{quote}

To answer this, we can make a table to describe it:

\begin{center}
  \begin{tabular} {|p{3cm}|p{2cm}|p{2cm}|}
    \hline
    \multirow{2}{*}{Bought Length} & \multicolumn{2}{c|}{Required length}\\
    \cline{2-3}
    & 5 meter & 1 meter\\ \hline
    11 meter\newline 6 meter & 2 per board\newline 1 per board & 1 per
    board\newline 1 per board\\ \hline
    Amount needed & 30 & 20\\ \hline
  \end{tabular}
\end{center}

This decomposes into the following equations:

\begin{itemize}
  \item $2x + y \leq 30$
  \item $x + y \leq 20$
  \item $x \geq 0$, $y \geq 0$
\end{itemize}

\marginpar{Notice how our shapes always have a convex hull...}

Plotting this on a graph gives us what's in Figure~\ref{fig:graph-4}, where we
can see a feasible region between $(0, 20), (10, 10), (15, 0)$ and  $(0,0)$. The
three corner points are all of the non-zero points (zero eleven meter and zero
six meter pieces is an invalid solution), and anywhere on the outer edge of the
region is a solution.

\begin{figure}[h]
  \centering
  \includegraphics{diagrams/graph4}
  \caption{A graph of $2x + y = 30$, $x + y = 20$, $x > 0$, $y > 0$.}
  \label{fig:graph-4}
\end{figure}

\subsection{The maximum problem}

Sometimes, we want to maximise some particular variable. For example, companies
want to maximise their profit. If this is the case, then we can come up with a
function for the profit, something like:

\[
  P = 7x + 3y
\]

Where $x$ is the number of $5$ meter boards, and $y$ is the number of $1$ meter
boards. We can factor this equation into our solution, to find the most
profitable wood to buy.

If you think about it, the further from the origin we get, the more profit we
get. At $(0,0)$ we have no profit, since we have done nothing, but at $(5,0)$ we
have $35$ profit since we have bought five, $5$ meter length boards. Since our
lines always intersect to produce a region with a convex shape, the points
furthest from the origin are the corner points. To figure out how much profit we
can make, we just need to find the maximum of these points:

\begin{center}
  \begin{tabular}{c | c | c | c}
    \textbf{Corner point} & \textbf{5 meter boards} & \textbf{1 meter boards}
      & \textbf{Profit}\\  \hline
    $(0,20)$  & 20 & 20 & 200\\ \hline
    $(10,10)$ & 30 & 20 & 270\\ \hline
    $(15,0)$  & 30 & 15 & 255\\
  \end{tabular}
\end{center}

Therefore the most profitable choice is to buy $10$ 11 meter length boards and
$10$ 6 meter length boards.

\subsection{The minimum problem}

This is similar to the maximum problem, except instead of maximising profit,
you'll be minimising cost, while still fulfilling some criteria. You plot a
graph, find the corner points of the feasible region and see which one minimises
the cost.

%TODO: Expand on this when you have a quiet and comfortable place to work...

\subsection{The Simplex Method}

If we have more than two variables, then all of a sudden, working out the
optimum on a graph becomes rather hard. Drawing 3d graphs is hard, and
visualising n-dimensional graphs quickly becomes infeasible.

The simplex method automates finding the optimum value of any number of
variables. This is how it works:

\begin{itemize}
  \item Draw the feasible region in $n$ dimensional space
  \item Select a corner of the feasible region
  \item Choose an edge that is connected to this point that will increase the
  objective function.
  \item Go to the next corner on that edge
  \item Keep doing steps 3 and 4 until you find an optimal solution.
\end{itemize}

That's pretty far removed from how we see the simplex algorithm working, since
we find the optimum by using a tableaux. I've broken down the algorithm into
multiple steps with a running example here:

\begin{itemize}
  \item[\textbf{Step 0}] The zero'th step, is to examine the input data we're 
  given. This will consist of a function $f$ to be maximised and a set of 
  inequalities that constrain the values we can give to $f$.

  \[\begin{split}
    f &= x + 2y + 3z\\
    4 &\geq x + y + z\\
    -1&\geq 2y -3z\\
    0 &\geq 2x -z
  \end{split}\]

  N.b. if the inequalities aren't of the form $CONST \geq EXPR$, then rearrange 
  them so that they are.

  \item[\textbf{Step 1}] Now we turn each inequality we were given into an 
  equation (replace the inequality sign with an equals basically), and introduce 
  a new variable for each (called a \textit{slack variable}). Also, rearrange
  $f$ so its equal to zero:

  \begin{alignat*}{12}
    4 &=~ &x~ &+~&y ~&+~&z ~&+~&m_1&~ &~  &~ &~  &~&~\\
    -1&=~ &~~ &+~&2y~&-~&3z~&~~&~  &+~&m_2&~ &~  &~&~\\
    0 &=~ &2x~&~~&~~ &-~&z ~&~~&~  &~ &~  &+~&m_3&~&~\\
    0 &=~ &-x~&-~&2y~&-~&3z~&~ &~  &~ &~  &~ &~  &+&f
  \end{alignat*}
  \item[\textbf{Step 2}] Now our problem is in the correct form, we can whack it 
  into a tableaux:

  \begin{center}
    \begin{tabular}{>{$}c<{$}|>{$}c<{$}|>{$}c<{$}|>{$}c<{$}|>{$}c<{$}|
      >{$}c<{$}|>{$}c<{$}|>{$}c<{$}|>{$}c<{$}}
          & x & y & z & m_1 & m_2 & m_3 & f & \text{constants}\\ \hline
      m_1 & 1 & 1 & 1 & 1   & 0   & 0   & 0   & 4\\ \hline
      m_2 & 0 & 2 &-3 & 0   & 1   & 0   & 0   &-1\\ \hline
      m_3 & 2 & 0 &-1 & 0   & 0   & 1   & 0   & 0\\ \hline
      f   &-1 &-2 &-3 & 0   & 0   & 0   & 1   & 0\\
    \end{tabular}
  \end{center}

  It should be quite obvious how we go from the equations in step 1 to the
  tableaux in here. The only stuff you need to \textit{remember} about
  formatting the tableaux, is that the slack variables go along the left side,
  $f$ is the bottom row, and to put the constants in the last column.

  The values in the bottom row are called \textbf{indicators}.

  \item[\textbf{Step 3}] We need to find the \textbf{pivot cell} in the table. 
  The pivot column is easy to find, its the one with the most negative 
  indicator (in our case, the third row, since the indicator is $-3$). 
  Finding the row is a bit harder, you need to divide the constant column by 
  the pivot column and pick the one with the lowest value:

  \definecolor{highlight}{gray}{0.7}
  \definecolor{dhighlight}{gray}{0.5}

  \begin{center}
    \begin{tabular}{>{$}c<{$}|>{$}c<{$}|>{$}c<{$}|
      >{$}>{\columncolor{highlight}}c<{$}|>{$}c<{$}|
      >{$}c<{$}|>{$}c<{$}|>{$}c<{$}|>{$}c<{$}|c}
          & x & y & z & m_1 & m_2 & m_3 & f & \text{constants}
            & \text{ratio}\\ \hline
      m_1 & 1 & 1 & 1                      &1&0& 0   & 0   & 4 & 4/1 \\ \hline
      m_2 & 0 & 2 &                      -3&0&1& 0   & 0   &-1 & -1/-3\\ \hline
      \rowcolor{highlight}
      m_3 & 2 & 0 &\cellcolor{dhighlight}-1&0&0& 1   & 0   & 0 & 0/0 \\ \hline
      f   &-1 &-2 &-3                      &0&0& 0   & 1   & 0 & 0/-3 \\
    \end{tabular}
  \end{center}

  \item[\textbf{Step 4}] Now we do the pivot about the pivot cell:

    \begin{itemize}
      \item[\textbf{Step 4a}] Divide each cell in the pivot's row by the pivot
      (which makes the pivot equal to $1$!).

      \begin{center}
        \begin{tabular}{>{$}c<{$}|>{$}c<{$}|>{$}c<{$}|>{$}c<{$}|>{$}c<{$}|
          >{$}c<{$}|>{$}c<{$}|>{$}c<{$}|>{$}c<{$}}
              & x & y & z & m_1 & m_2 & m_3 & f & \text{constants}\\ \hline
          m_1 & 1 & 1           & 1&1&0            &0&0& 4\\ \hline
          m_2 & 0 & 2 &                      -3&0&1& 0   & 0   &-1\\ \hline
          m_3 &-2 & 0 &\cellcolor{dhighlight} 1&0&0&-1   & 0   & 0\\ \hline
          f   &-1 &-2           &-3&0&0            &0&1& 0\\
        \end{tabular}
      \end{center}

      \item[\textbf{Step 4b}] Subtract the pivot row from each other row
      so that the whole pivot column becomes $0$. For example here, we need to
      $R1 = R1 - R3, R2 = R2 + 3(R3), R4 = R4 + 3(R3)$ to make the cell below 
      the pivot $0$.

      \begin{center}
        \begin{tabular}{>{$}c<{$}|>{$}c<{$}|>{$}c<{$}|>{$}c<{$}|>{$}c<{$}|
          >{$}c<{$}|>{$}c<{$}|>{$}c<{$}|>{$}c<{$}}
              & x & y & z & m_1 & m_2 & m_3 & f & \text{constants}\\ \hline
          m_1 & 3 & 1 &  0 & 1 & 0 & 1 & 0 & 4\\ \hline
          m_2 &-6 & 2 &  0 & 0 & 1 & -3 & 0 &-1\\ \hline
          m_3 &-2 & 0 &  1 & 0 & 0 & -1& 0 & 0\\ \hline
          f   &-7 &-2 &  0 & 0 & 0 & -3& 1 & 0\\
        \end{tabular}
      \end{center}

      \item[\textbf{Step 4c}] Replace the letter on the pivot row with the 
      letter on the pivot column:

      \begin{center}
        \begin{tabular}{>{$}c<{$}|>{$}c<{$}|>{$}c<{$}|>{$}c<{$}|>{$}c<{$}|
          >{$}c<{$}|>{$}c<{$}|>{$}c<{$}|>{$}c<{$}}
              & x & y & z & m_1 & m_2 & m_3 & f & \text{constants}\\ \hline
          m_1 & 3 & 1 &  0 & 1 & 0 & 1 & 0 & 4\\ \hline
          m_2 &-6 & 2 &  0 & 0 & 1 & -3 & 0 &-1\\ \hline
          z   &-2 & 0 &  1 & 0 & 0 & -1& 0 & 0\\ \hline
          f   &-7 &-2 &  0 & 0 & 0 & -3& 1 & 0\\
        \end{tabular}
      \end{center}
    \end{itemize}

    \item[\textbf{Step 5}] Now we have found another solution, to see what it 
    is, read off the letters on the far left and correspond them to the 
    constants on the far right:

    \[
      m_1 = 4, z = 0, m_2 = -1, f = 0
    \]

    If we sub that in, then we can see that if satisfies the formulae from step
    1:

    \begin{alignat*}{12}
      4 &=~ &0~&+~&0~&+~&0~&+~&4&~ &~ &~ &~&~&~\\
      -1&=~ &0~&+~&0~&-~&0~&~~&~&+~&-1&~ &~&~&~\\
      0 &=~ &0~&~~&0~&-~&0~&~~&~&~ &~ &+~&0&~&~\\
      0 &=~ &0~&-~&0~&-~&0~&~ &~&~ &~ &~ &~&+&0
    \end{alignat*}

  \item[\textbf{Step 6}] If there are no negative indicators in the table
  (bottom row of the tableaux), then we've finished, and our solution is the 
  optimal one. If there are, then we need to go back to step 3 and repeat until 
  there aren't any negative indicators.

\end{itemize}

If we run steps 3-6 again, we should eventually get the optimum, which according
to \url{http://www.zweigmedia.com/RealWorld/simplex.html} is $f = 12; x = 0, y =
0, z = 4$.

\section{Turing machines and complexity measures}

A Turing machine is a theoretical device that implements a model of computation.
It operates on a tape of infinite length, which is divided into cells, and can
read and write symbols from/to each cell when the \textit{head} of the machine
is positioned over the cell. The Turing machine also has internal state, which
is finite and determines what actions it takes (reading, writing, moving the
tape etc).

In this course, we will be thinking about multi-tape Turing machines. Here, the
there are many tapes, numbered $1 \dots K$. Tape $1$ is the input tape and tape
$K$ is the output tape. The tapes inbetween the input and output tapes are the
work tapes. Any algorithm can be expressed as a multi-tape Turing machine

\subsection{Formally defining a Turing Machine}

A TM (Turing Machine) can be define as a quintuple, as shown in
Figure~\ref{fig:tm-definition}.

\begin{figure}[H]
  \centering
  \includegraphics{equations/tm-definition}
  \caption{Part of a multi-tape Turing machine definition. We must also define
  the word \textit{symbol}, which is a mark occupying one cell on the tape. The
  alphabet that a symbol can be from is $\Sigma \cup \{
  \textrm{\textvisiblespace}, \vartriangleright\}$, where \textvisiblespace~is a
  blank cell, and $\vartriangleright$ is the start symbol (signifying the left
  edge of the tape).}
  \label{fig:tm-definition}
\end{figure}

Up to now, we've defined a TM that is perfectly formed\dots but does nothing. We
need to define how it moves between states in $Q$. To do that, we define a
transition, as in Figure~\ref{fig:tm-transition}.

\begin{figure}[H]
  \centering
  \includegraphics{equations/tm-transition}
  \caption{A transition of a Turing Machine}
  \label{fig:tm-transition}
\end{figure}

Informally, this transition applies only if the current state is $p$, and the
squares on the tapes confirm to $\bar{s}$, then set the new state to $q$ and
write the symbols from $\bar{t}$ to the tapes and move the heads as directed by
$\bar{d}$ (Left, Right or Stay).

Now we've nearly finished defining a multi-tape TM, the final points are:

\begin{itemize}
  \item Since the TM is deterministic, there can be \textit{at most} one 
    transition for every $(state,~\bar{current~symbols})$ tuple.
  \item The tape never moves left $\vartriangleright$.
  \item Tape 1 is read only and tape $K$ is write only.
\end{itemize}

\subsubsection{Acceptance, computability and decidability}

We say that a TM $M$ \textit{accepts} an input $x$ if it halts on $x$. The
language \textit{recognised} my $M$ is the set of strings that is accepted by
$M$.

\marginpar{Remember, when we say `Turing Machine', we could mean either a 
deterministic TM or a non-deterministic one (unless we explicitly said which one
we meant of course!).}

If a language is recognised by a Turing Machine $M$, then there is also a 
deterministic Turing Machine $M'$ that will recognise the same language. 

A language is \textit{computable} (aka decidable), if there is a (deterministic)
TM that will accept every string in the language, and reject every string not in
the language.

Up to now, our definitions of recognisable and computable are pretty similar (if
not the same). The difference is, that if an input string \textbf{isn't} in a
computable language, then the TM will halt (and output `no'), whereas if the
input string isn't in a recognisable language, then the TM \textit{may} halt and
output `no', or it could just carry on computing for ever. All a Turing Machine
does with a recognisable language, is recognise if a string is in the language,
it makes no guarantees about string not in the language!

\subsubsection{The Universal Machine}

If you're that way inclined, you can find lots of cool things about Turing
Machines. Another such cool thing, is that every TM is a finite thing (it's just
a quintuple as shown in Figure~\ref{fig:tm-definition}), which means we can come
up with a way of encoding any Turing Machine into a string over the alphabet
$\Sigma$ and use it as input to another Turing Machine!

If we can do this, then we can construct a \textbf{Universal Machine} $U$. This
machine takes two `arguments', another TM $M$ (encoded of course) and some
arbitrary input $x$. If $M \downarrow x$, then $U$ finishes with $y$ on its
output tape. If $M \uparrow x$, then $U$ will never terminate. The input is
coded as $M;x$, where `;' is a separator between the Turing Machine we're
`simulating' and the input we'll give it.

\subsubsection{Nailing that TM definition}

If you want your Turing Machines better defined than Arnie's Biceps, then this
how you do it (alternately, read Chapter 3 of Sipser).

%TODO: Not sure about a configuration on slide 8 of lecture 7b. I emailed Ian.

A run of a TM is terminating if it is finite. If the input to a deterministic TM
$M$ is $x$, and the run is finite, we write $M \downarrow x$ otherwise, if the
run is infinite, we write $M \uparrow x$.

Let $M$ be a deterministic TM that knows the alphabet $\Sigma$, and its input be
$x \in \Sigma*$ (i.e. the input is zero or more symbols from the alphabet). If
$M \downarrow x$ then the output tape of $M$ will contain some string $y \in
\Sigma*$ once M has finished computing. The cool thing, is that we can treat $M$
like a function, lets call it $f_M$, that operates over $\Sigma* \rightarrow
\Sigma*$:

\[
  f_M(x) = \begin{cases}
       y & if M \downarrow x\\
       \text{undefined} & if M \uparrow x\\
     \end{cases}
\]

In this case, $M$ computes the function $f_M$.

\subsubsection{Enumerators}

Enumerators aren't in the course slides, but they do appear in the reading. Skip
this section if you're doing last minute cramming.

An Enumerator is a Turing Machine with an attached printer. Now, I know what
you're thinking; \textit{I'm a computer scientist, I hate printers. Why would
anybody combine a complicated enough idea like a TM with a printer?!}
Enumerators have an important theoretical use though, and don't require us to
install drivers like we do for printers, and they certainly don't need to work
over WiFi (so this section should be a piece of cake really).

The TM inside an enumerator is able to send a string to the printer whenever it
wants to print out a string. If the TM does not halt, then the printer may print
an infinite list of strings. The language that is \textit{enumerated} by the
enumerator, is the collection of strings that it eventually prints out. Since
there are no rules about repetitions or ordering in the output list of the
printer, you might want to think about the enumerated language as a set.

So, as I said before, enumerators \textit{are} useful. Namely a language is
Turing recognisable if and only if, there is some enumerator that enumerates it.

Its easy to show that a TM can recognise a language that is enumerated by an
enumerator, just build a TM that does this:

\begin{enumerate}
  \item We have an input word $w$
  \item Run the enumerator $E$:
  \begin{enumerate}
    \item Every time $E$ outputs a new string, compare it to the word $w$
    \item If they are equal, then accept the word.
  \end{enumerate}
\end{enumerate}

Note that there is the potential for this TM to run for an infinite length of
time, if the enumerator generates an infinite list, and $w$ is not in the
language it enumerates.

We can also make an enumerator enumerate any language that is recognised by a
Turing Machine $M$:

\begin{enumerate}
  \item Ignore any input
  \item For $i = 1$ to $i = \infty$
  \begin{enumerate}
    \item Run the TM $M$ for $i$ steps for all possible strings of length $i$ in
      the language ($\Sigma^i$)
    \item If any computations accept, then print out the corresponding string.
  \end{enumerate}
\end{enumerate}

This enumerator will never halt (since it loops infinitely many times over an
infinitely large set of input strings), but eventually, it will print out all of
the words in the language (even though there will be \textit{a lot} of
duplicates!).

\subsection{The Halting problem}

The Halting problem asks:

\begin{quote}
  Given a Turing Machine $M$ and a string for its input $x$, return:
  \begin{itemize}
    \item[] \textbf{Yes} if $M \downarrow x$ ($M$ decides $x$)
    \item[] \textbf{No} otherwise, where $M$ would compute forever
  \end{itemize}
\end{quote}

Turing managed to prove that there is no Turing Machine that will decide the
Halting problem. The proof is fairly simple, but it takes a while to get your
head around it when its written in a mathematical form. Luckily, I found a poem,
written in the style of \href{https://en.wikipedia.org/wiki/Dr._Seuss}{Dr.
Seuss}, that provides a better textual explanation than I ever could; see
Figure~\ref{fig:halting-poem}.

\begin{figure}[h]
\begin{minipage}{\textwidth}
  \newgeometry{top=2cm,bottom=2cm,left=1cm,right=3cm}
  \begin{mymulticols}
  \input{poem.tex}
  \end{mymulticols}
  \restoregeometry
\end{minipage}
\caption{A poetic explanation of the Halting Problem.}
\label{fig:halting-poem}
\end{figure}

As nice as it is, I think it's unlikely that the poem would be accepted as an
answer in an exam, so we'd better learn the formal definition of the Halting
problem too.

Suppose we have a TM $P$ that can determine if another TM can halt. We're going
to make another TM $Q$ that is defined so:

\begin{enumerate}
  \item Duplicate the input $x$ TM we're given.
  \item Run $P$ on the duplicated input ($x;x$).
  \begin{itemize}
    \item[] $P(x;x) = Y$, then Loop 
    \item[] $P(x;x) = N$, then Halt
  \end{itemize}
\end{enumerate}

Now, if we give $Q$ the input $Q$, then the embedded $P$ will receive $(Q,Q)$,
meaning that if:

\begin{description}
  \item $Q \downarrow Q \implies Q \uparrow Q$
  \item $Q \uparrow Q \implies Q \downarrow Q$
\end{description}

% TODO: Find another explination of the halting problem

\subsection{Complexity}

You already know that there are two types of Turing Machine; deterministic and
non-deterministic TM's, but now its time to think about the resources that Turing
Machines consume. We don't think of TM resources as commodities such as metal,
electricity etc, since TM's are abstract, theoretical devices. Instead the two
commodities we're interested in with Turing Machines are \textbf{time} and
\textbf{space}.

%TODO: Read chapter 4 of Sipser, carry of from slide 18 of 
% http://studentnet.cs.manchester.ac.uk/ugt/2015/COMP36111/lecture7b.pdf