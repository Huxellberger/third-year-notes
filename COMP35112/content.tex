% Set the author and title of the compiled pdf
\hypersetup{
  pdftitle = {\Title},
  pdfauthor = {\Author}
}

\section{The need for parallelism}

Even though we've been unable to increase the clock speed of processors since
around 2005, we have seen the `power' of processors roughly double every 18-24
months since then in line with Moore's Law. The reason why, is that we have been
able to increase the amount of transistors in chips (due to the feature size
decreasing), and use the extra ones to provide more processing cores, which are
able to process data in parallel. The degree of parallelism is increasing as
time progresses.

In an ideal world providing a greater degree of parallelism would merely entail
chip designers copy and pasting multiple processor cores onto the silicon, and
programmers getting linear performance increases. In practice, there are lots of
architectural issues (such as how processors are connected and how they're
organised) as well as software issues (how do we make our app run on multiple
cores).

As transistors are becoming smaller, we can also make them switch faster. The
switching speed is determined by $R*C$, where $R$ is the resistance and $C$ is
the capacitance. When we reduce the area of the transistor, $C$ decreases, so in
doing so, we make the circuit able to compute faster.

This was fine until 2005, at which point we started to have problems with things
like \textit{interconnect capacitance} (the capacitance between neighbouring
wires). Also, the power density increased (smaller transistors means more per
unit area), which has serious cooling implications. As we approach the
theoretical limit of one atom per transistor, any impurity in the silicon
becomes a major issue.

We have tried using extra transistors to build more complex single core
processors (using Instruction Level Parallelism (ILP)) and by adding bigger
caches so that they exhibit lower miss rates, however both of these techniques
suffer from diminishing returns. Control statements such as \texttt{branches}
make us have to periodically throw away all the partially completed instructions
in a pipeline, and caches already have hit-rates in the high $90$'s.

Though we might be able to increase the number of cores on a chip, how does a
programmer utilise this extra power? It is relatively easy for an operating
system to schedule programs so that they can run on different cores and
therefore have true multi-tasking (process level parallelism), but what if we
want to make one program run faster by running it over many cores?

\subsection{ILP vs TLP}

Instruction Level Parallelism and Thread Level Parallelism are two different
approaches to utilising parallel hardware, and both can be used at the same
time.

In ILP, the processor is able to execute instructions out of order and in
parallel, meaning that fewer clock cycles are needed to execute the same number
of instructions. This form of parallelism is very limited, and can only be used
in certain situations. Vector parallelism is similar, and lets you do things
like do four 8-bit additions in one instruction (by splitting a 32-bit word into
four 8-bit parts).

In TLP, a program can be composed of separate threads, each
being its own sequence of instructions. Many threads can be executing in
parallel and since their instructions are independent of each other, can be
interleaved on the processor (or run on multiple cores). The output of threaded
programs must be deterministic, i.e. for all possible sequences of execution,
the output must be the same.

TLP is far more general purpose than ILP (and much more so than vector
parallelism, which is only really useful in simple operations like array
addition). However, it relies on the programmer finding a way to express an
algorithm in a parallel manner.

\subsection{Data and instruction parallelism}

\textit{Flynn} classified parallelism as \textbf{instruction stream} and
\textbf{data stream} parallelism.

Data parallelism is when the same computation is carried out on multiple
elements of some dataset, usually an array. Vector parallelism is an example of
this. Only certain problems amenable to this type of parallelism can be sped up
in this manner.

Instruction parallelism is when multiple instructions can be executed in
parallel (with no side effects that cause problems to each other).

Different combinations of these types of parallelism have different names:

\begin{description}
  \item \textbf{SISD}: Single Instruction Single Data is like a normal program.
  A serial sequence of executions working on a stream of data, one word at a
  time.
  \item \textbf{MIMD}: Multiple Instruction Multiple Data is the most parallel
  one, where there are multiple instruction streams and they can all operate on 
  their own independent stream of data. This is most normal computer chips.
  \item \textbf{SIMD}: Single Instruction Multiple Data is stuff like when you 
  use instructions like \texttt{ADD8} to process multiple data elements at once 
  with the same instruction, or a GPU processing lots of pixels at once with 
  the same filter. If it's not a vector instruction, then it's one or more 
  processors working in lockstep to process elements of an array or something.
  \item \textit{SPMD}: Single Program Multiple Data is a generalisation of SIMD,
  where different processors execute the same code but don't need to be in 
  lockstep. This is most often how parallel programs are written for CPU's.
\end{description}

\subsection{Connecting processors}

In order to make them work in parallel, multiple cores of a processor need to be
connected to each other, and to memory. There are lots of different ways to do
this, and the best way depends on the use case.

\begin{description}
  \item The processors can be laid out in a grid. In this case, each processor
  can communicate with its neighbours, and memory is usually private to each 
  core. In order for cores to access memory in other cores, they must send 
  messages through the grid.
  \item A Torus is a variation of the grid, where each edge is linked to the 
  opposite edge. This makes a doughnut shape (in a logical, not physical sense)
  and means that fewer steps are needed to communicate between cores.
  \item A bus can be used to connect multiple cores. The memory is usually
  situated on the bus, and all cores have access to it (though they may also
  have their own memory instead). Time slicing is used to give equal access to 
  the bus, but can make the bus become a processing bottleneck.
\end{description}

There are, of course, more types of interconnects. Crossbars are where each node
is connected to each other node (which has a complexity of $n(n-1)$), but the
best ones are usually tree structures or hierarchical buses where the complexity
is logarithmic ($n log(n)$).

\subsection{Shared and distributed memory}

Shared memory is accessible from all of the cores and every part of the
computation, while distributed memory is spread out in different components, and
is usually only accessible by the component that owns it. We are considering
systems that are either one of these, or the other. Either one of these can
emulate the other from a software point of view; it is fairly easy to provide
abstraction layers that make a distributed memory behave like a shared one, or
impose restrictions on shared memory so that parts are unavailable to certain
components. Imposing a foreign memory layout onto the hardware comes at a
performance penalty.

Most supercomputers use distributed memory, since its easier to build, provides
a higher total communication bandwidth and is more suited to many data-parallel
problems. However, programs using data sharing (shared memory) are widely seen
to be easier to code than programs using message passing (distributed memory),
so there is an overhead involved with distributed memory.

% Lecture 3

\section{Using threads}

A thread is a flow of control executing a program, and a process can consist of
one or more threads. Each thread inside a process has access to the same
address space, and most programming languages provide some method of using
threads.

Now, go and look up Java threads
(\url{https://docs.oracle.com/javase/tutorial/essential/concurrency/}) and C's
pThreads (\url{https://computing.llnl.gov/tutorials/pthreads/}).

The easiest form of parallelism to find and exploit using threads is data
parallelism. This is where computation is divided into roughly equal chunks,
where hopefully, each chunk is independent of the next.

An example of this is multiplying two $n\times n$ matrices. We could use $n^2$
threads, each computing one element (remember the area of the output matrix is
going to be $n^2$), or we could use $n$ threads and have each thread compute a
whole row or column. If we didn't have that many threads (or perhaps making more
threads is inefficient), then we could make $p$ threads and have each one
compute $q$ columns or rows, where $p\times q = n$.

In Fortran, \texttt{DOALL} statements let us execute the body of a loop in
parallel.

There are two types of parallelism:

\begin{description}
  \item \textbf{Implicit:} This is when the system works out how the parallelise
  something for itself. This is particularly relevant to functional languages,
  since many operations (map, reduce etc) are inherently parallelisable.
  \item \textbf{Explicit:} Here, the programmer must have a mental model of the
  parallelism in his or her head, and specifies exactly what should be done.
\end{description}

A lot of the time, parallel programs are made in a way that blends the two (so
you might not have to specify everything explicitly, but you have to give hints
to the system as to what should be parallelised and how). An example of this is
the \texttt{DOALL} statement mentioned above.

In an ideal world, we would have computers that would automatically parallelise
programs. However, since dependency analysis is hard to do, this approach is
limited. Computers do automatic parallelisation to an extent (e.g. instruction
reordering), but must be $100\%$ sure that an operation is safe to parallelise,
and the results will be correct.

\section{Caches in shared memory multiprocessors}

Obviously caches are vital to the efficient functioning of processors. Without
them, every memory access would cause the CPU to wait around 200 cycles, and so
having a cache is vital to having the CPU run at close to full speed.

However, caches don't just fix the problem; we need to work out how to populate
them, and keep the data in them correct. Data that we write to a cache must
eventually be written back to memory, and new memory locations need to be loaded
into the cache when they're required by the CPU.

We solved these problems in \texttt{COMP25111}, however, more problems arise
when you consider a multi-core processor. In most multi-core processors, each
core has its own cache, and since each cache can potentially hold its own copy
of the same memory location, we need to make sure that they agree with each
other about the values of these locations. This is the \textbf{cache coherence
problem.}

An easy solution to this would be to require that every write would go through
to memory straight away, and the other cache(s) in other cores would load the
value. This is obviously slow though and there may be bus bandwidth problems.
Furthermore, we'd only be getting a benefit from cache-reads, which defeats
(half of) the object of the cache!

We can overcome this by making the caches talk to each other. When a new value
is written to one cache, the others should invalidate their own cache lines
containing this value. This means a write to a cache doesn't need to go straight
through to memory, but just flips a bit inside the other caches.

However, when we introduce more state to our caches (such as invalidated cache
lines) we also increase the complexity, and need a model to make sure things
don't go wrong. Each cache line can be in three states:

\begin{description}
  \item Invalid; There might be an address math on this line, but the data is 
  not valid any more. It needs to be fetched from memory again.
  \item Dirty; The cache line is valid, and has been update in the cache since 
  it was loaded from memory. It must be written to memory at some point in the 
  future.
  \item Valid; The cache line matches what's currently in memory.
\end{description}

In order to let caches know what other core's caches are doing, we have them do
\textbf{bus snooping}. This involves having hardware watch each core's cache and
modify the cache independently of the core so that the flags on the cache lines
are correct.

Given two cores, the following states are valid and invalid:

\begin{center}
  \begin{tabular}{c|c c c}
      & V & D & I\\ \hline
    V & T & F & T\\
    D & F & F & T\\
    I & T & T & T
  \end{tabular}
\end{center}

Notice how the table is a mirror image of itself. We can't have two dirty
states, since then we won't know which we should write to memory, and we can't
have a dirty and valid state (since then, by definition, the valid state is not
valid).

% TODO: Talk about state transitions

% Slide 22 onwards

There are two types of messages between cores; read requests for a cache line
(one core hopes that another core's cache has the cache line so that it doesn't
have to go to memory), and invalidate messages.

We can easily extend the protocol we've described beyond two cores; any core
with a valid value can respond to read requests (the bus will decide who
`wins'), and invalidate requests work as normal, invalidating the cache line on
all cores.

The only extra requirement is that invalidation must happen in one cycle, since
we want all cores to have the same view of memory, and if one core receives the
invalidation message after another, there will be a period of time where their
views of memory will be inconsistent. This gets harder as we increase the number
of cores; the bus gets longer and so slows down (signals take longer to
propagate, and the clock will have to be reduced).

The impact from this is that the consistency protocol is the biggest limitation
when trying to add more cores to a processor. The protocol we have described is
called the \texttt{MSI protocol} (modified, shared, invalid).

\subsection{Other cache coherence protocols}

In the previous cache coherence protocols we have discussed, we came across
situations where there was unneccecary bus usage (e.g. when one core writes to
its cache and makes a cache line dirty, other cores would write their copies
back to memory, even though that value would never be used since there was a
newer dirty version).

However, the bus is a \textit{critical shared resource}, and we certainly don't
want to waste bandwidth on messages that have no effect. We can distinguish
between two cases of writing to a cache:

\begin{itemize}
  \item When the cache holds the only copy of a value, and it's not dirty (if it
  was dirty, we'd just update it).
  \item When the cache holds a copy of the value, but there are other copies in
  other caches.
\end{itemize}

It is only the first case where we don't need to send an invalidate message, but
this is nevertheless a common case. In most multithreaded programs, only a
minority of memory locations are shared between threads (and a smaller minority
are both read and written to by multiple threads), so the majority of memory
locations are unshared. If we split the `V' (valid) state into two more states,
we can account for this case:

\begin{itemize}
  \item \textbf{E} - Exclusive to one cache
  \item \textbf{S} - Shared between multiple caches
\end{itemize}

This gives us the \texttt{MESI} protocol (as opposed to the \texttt{MSI}
protocol). This is easy to implement on top of the \texttt{MSI} protocol; simply
set the state to E if the value was read from memory, and to S if it was read
from another cache.

Though minor changes in the protocol are required, the only inconsistency is
that if all E/S lines are evicted from other caches except one S line, then the
S line is now exclusive (even though it is marked as S). This isn't a problem in
practice, since it doesn't happen often, and since it's hard to detect, it's
just ignored.

The \texttt{MOSEI} protocol is a further optimisation, where the M state is
split into:

\begin{itemize}
  \item \textbf{M} - Modified; the cache contains a copy which differs from 
  memory, and no other caches contain a copy.
  \item \textbf{O} - Owned; the cache contains a copy which differs from the one 
  in memory, and some other caches also contains a copy, but those are in state 
  S and have the same value as the owner.
\end{itemize}

With the additional O state, we can share the latest value and don't have to
write it back to memory straight away. Only when a cache line in the O or M
states is evicted, will any writing to memory be done.

\subsection{Directory based coherence}

So far, we've looked at methods for letting multiple cores communicate over a
single bus, and have assumed that bus communication happens instantaneously.
However, as the number of cores increases, the bus capacitance increases and you
have to slow it down. This limit is at around 32 cores.

A directory based coherence method is an attempt to make everything less
directly connected to get around this limitation. A centralised directory is
created that holds information about each cache line (which contains multiple
words) in memory.

For each cache line, the directory contains a bit map for which core has a copy,
and whether that copy is dirty, which is another bit map (though only one bit is
true). Each cache has its own valid and dirty bits, which are used to dictate
whether to write back to memory or not. If a core wahts to make a memory access,
we have the following cases:

\begin{description}
  \item \textbf{Read hit in local cache}:\\
    Just read the local value!
  \item \textbf{Read miss in the local cache}:\\
    Ask the directory:
    \begin{description}
      \item \textbf{Directory dirty bit is false}:\\
        Read the data from main memory into the cache, set the directory
        presense and local valid bit for that core.
      \item \textbf{Directory dirty bit is true}:\\
        Cause the owner to write back the value to memory, which is also sent to
        the cache that was asking. The directory dirty bit is cleared, but the
        directory presence bits are set, as is the local valid bit.
    \end{description}
  % TODO: Carry on from slide 11
\end{description}
